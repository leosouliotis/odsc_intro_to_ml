{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning in Python\n",
    "\n",
    "Welcome to this introductory session on Machine Learning with Python! Machine learning is a powerful tool that allows computers to learn from data and make predictions or decisions without being explicitly programmed. This technology is at the heart of many modern applications, from recommendation systems and image recognition to financial forecasting and autonomous driving.\n",
    "\n",
    "In this session, we’ll focus on understanding the basic concepts of machine learning and how to implement them using Python. We’ll start with the fundamental principles, explore essential algorithms, and use Python’s powerful libraries to build and evaluate simple machine-learning models. By the end of this workshop, you will have a foundational understanding of machine learning and the practical skills to start applying these techniques to your own projects.\n",
    "\n",
    "Here is an outline of what we will talk about today:\n",
    "\n",
    "1. [Introduction to Machine Learning](#Introduction-to-Machine-Learning)\n",
    "2. [Data Preparation and Exploration](#Data-Preparation-and-Exploration)\n",
    "3. [Introduction to Supervised Learning](#Intoduction-to-Supervised-Learning-in-Python)\n",
    "4. [Q&A](#Q&A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "Machine learning (ML) is the study of computer algorithms that can improve automatically through experience and by the use of data.\n",
    "\n",
    "The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables.\n",
    "\n",
    "Machine learning is all about results, it is likely working in a company where your results are characterized solely by your how good they are. Whereas, statistical modeling is more about finding relationships between variables and the significance of those relationships, whilst also catering for prediction.\n",
    "\n",
    "![ml_buldozer](img/ml_buldozer.png)\n",
    "\n",
    "[*Source*](https://xkcd.com/1838/)\n",
    "\n",
    "## Types of machine learning problems\n",
    "Not all machine learning problems are created equal, nor they use the same methods to make predictions. Given the problems nature, available data and the reserahcer's goal, a machine learning problem can fall into one of the following categories:\n",
    "\n",
    "### Supervised learning\n",
    "\n",
    "Supervised learning (SL) is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. In supervised learning, each example is a pair consisting of an input and a desired output value. A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations.\n",
    "\n",
    "The most widely used learning algortithms are:\n",
    "\n",
    "  - Support-vector machines\n",
    "  - Linear regression\n",
    "  - Logistic regression\n",
    "  - Naive Bayes\n",
    "  - Decision trees\n",
    "  - K-nearest neighbor algorithm\n",
    "  - Neural networks (Multilayer perceptron)\n",
    "  \n",
    "Supervised learning methods are used for problems like:\n",
    "  - Classification; assigning a categorical laber to each observation\n",
    "  - Regression; assigning a numeric value to each observation\n",
    "\n",
    "### Unsupervised learning\n",
    "\n",
    "Unsupervised learning (UL) is a type of machine learning in which the algorithm is not provided with any pre-assigned labels or scores for the training data. As a result, unsupervised learning algorithms must first self-discover any naturally occurring patterns in that training data set. Advantages of unsupervised learning include a minimal workload to prepare and audit the training set, in contrast to supervised learning techniques where a considerable amount of expert human labor is required to assign and verify the initial tags, and greater freedom to identify and exploit previously undetected patterns.\n",
    "\n",
    "The most widely used learning algortithms are:\n",
    "\n",
    "  - Hierarchical clustering\n",
    "  - K-means\n",
    "  - Mixture models\n",
    "\n",
    "Supervised learning methods are used for problems like:\n",
    "  - Clustering; find the optimal way to slit the data into distinct groups\n",
    "  - Dimensionality reduction; project the data in a lower dimensional space\n",
    "\n",
    "### Reinforcement learning\n",
    "Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning differs from supervised learning in not needing labelled input/output pairs be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Exploration\n",
    "\n",
    "Data preparation is a critical step in any machine learning workflow. The quality and structure of your data directly affect the performance of your model. In this section, we will cover the essential steps for preparing your data for machine learning:\n",
    "\n",
    "1. Loading datasets in Python.\n",
    "2. Cleaning data by handling missing values and removing duplicates.\n",
    "3. Performing Exploratory Data Analysis (EDA) to understand the data.\n",
    "4. Feature engineering, including feature scaling and encoding categorical variables.\n",
    "\n",
    "Let's get started with loading a dataset. In this section we will work with the [*heart disease*](https://archive.ics.uci.edu/dataset/45/heart+disease) dataset, where the goal is to predict the precense of heart disease (values >0) or not (0).\n",
    "\n",
    "Let's explore our dataset!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets in Python\n",
    "\n",
    "Some datasets can be more easily loaded, *without the need to downalod the data in our local machine.* For this example, our dataset is stored on the [UC Irvine ML Repository](https://archive.ics.uci.edu) and we can download the data like that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   1       145   233    1        2      150      0      2.3      3   \n",
       "1   67    1   4       160   286    0        2      108      1      1.5      2   \n",
       "2   67    1   4       120   229    0        2      129      1      2.6      2   \n",
       "3   37    1   3       130   250    0        0      187      0      3.5      3   \n",
       "4   41    0   2       130   204    0        2      172      0      1.4      1   \n",
       "\n",
       "    ca  thal  \n",
       "0  0.0   6.0  \n",
       "1  3.0   3.0  \n",
       "2  2.0   7.0  \n",
       "3  0.0   3.0  \n",
       "4  0.0   3.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "dataset = heart_disease.data\n",
    "dataset.features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of the second method is that our dataset is a more complex Python object with more metadata about the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uci_id': 45,\n",
       " 'name': 'Heart Disease',\n",
       " 'repository_url': 'https://archive.ics.uci.edu/dataset/45/heart+disease',\n",
       " 'data_url': 'https://archive.ics.uci.edu/static/public/45/data.csv',\n",
       " 'abstract': '4 databases: Cleveland, Hungary, Switzerland, and the VA Long Beach',\n",
       " 'area': 'Health and Medicine',\n",
       " 'tasks': ['Classification'],\n",
       " 'characteristics': ['Multivariate'],\n",
       " 'num_instances': 303,\n",
       " 'num_features': 13,\n",
       " 'feature_types': ['Categorical', 'Integer', 'Real'],\n",
       " 'demographics': ['Age', 'Sex'],\n",
       " 'target_col': ['num'],\n",
       " 'index_col': None,\n",
       " 'has_missing_values': 'yes',\n",
       " 'missing_values_symbol': 'NaN',\n",
       " 'year_of_dataset_creation': 1989,\n",
       " 'last_updated': 'Fri Nov 03 2023',\n",
       " 'dataset_doi': '10.24432/C52P4X',\n",
       " 'creators': ['Andras Janosi',\n",
       "  'William Steinbrunn',\n",
       "  'Matthias Pfisterer',\n",
       "  'Robert Detrano'],\n",
       " 'intro_paper': {'title': 'International application of a new probability algorithm for the diagnosis of coronary artery disease.',\n",
       "  'authors': 'R. Detrano, A. Jánosi, W. Steinbrunn, M. Pfisterer, J. Schmid, S. Sandhu, K. Guppy, S. Lee, V. Froelicher',\n",
       "  'published_in': 'American Journal of Cardiology',\n",
       "  'year': 1989,\n",
       "  'url': 'https://www.semanticscholar.org/paper/a7d714f8f87bfc41351eb5ae1e5472f0ebbe0574',\n",
       "  'doi': None},\n",
       " 'additional_info': {'summary': 'This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them.  In particular, the Cleveland database is the only one that has been used by ML researchers to date.  The \"goal\" field refers to the presence of heart disease in the patient.  It is integer valued from 0 (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).  \\n   \\nThe names and social security numbers of the patients were recently removed from the database, replaced with dummy values.\\n\\nOne file has been \"processed\", that one containing the Cleveland database.  All four unprocessed files also exist in this directory.\\n\\nTo see Test Costs (donated by Peter Turney), please see the folder \"Costs\" ',\n",
       "  'purpose': None,\n",
       "  'funded_by': None,\n",
       "  'instances_represent': None,\n",
       "  'recommended_data_splits': None,\n",
       "  'sensitive_data': None,\n",
       "  'preprocessing_description': None,\n",
       "  'variable_info': 'Only 14 attributes used:\\r\\n      1. #3  (age)       \\r\\n      2. #4  (sex)       \\r\\n      3. #9  (cp)        \\r\\n      4. #10 (trestbps)  \\r\\n      5. #12 (chol)      \\r\\n      6. #16 (fbs)       \\r\\n      7. #19 (restecg)   \\r\\n      8. #32 (thalach)   \\r\\n      9. #38 (exang)     \\r\\n      10. #40 (oldpeak)   \\r\\n      11. #41 (slope)     \\r\\n      12. #44 (ca)        \\r\\n      13. #51 (thal)      \\r\\n      14. #58 (num)       (the predicted attribute)\\r\\n\\r\\nComplete attribute documentation:\\r\\n      1 id: patient identification number\\r\\n      2 ccf: social security number (I replaced this with a dummy value of 0)\\r\\n      3 age: age in years\\r\\n      4 sex: sex (1 = male; 0 = female)\\r\\n      5 painloc: chest pain location (1 = substernal; 0 = otherwise)\\r\\n      6 painexer (1 = provoked by exertion; 0 = otherwise)\\r\\n      7 relrest (1 = relieved after rest; 0 = otherwise)\\r\\n      8 pncaden (sum of 5, 6, and 7)\\r\\n      9 cp: chest pain type\\r\\n        -- Value 1: typical angina\\r\\n        -- Value 2: atypical angina\\r\\n        -- Value 3: non-anginal pain\\r\\n        -- Value 4: asymptomatic\\r\\n     10 trestbps: resting blood pressure (in mm Hg on admission to the hospital)\\r\\n     11 htn\\r\\n     12 chol: serum cholestoral in mg/dl\\r\\n     13 smoke: I believe this is 1 = yes; 0 = no (is or is not a smoker)\\r\\n     14 cigs (cigarettes per day)\\r\\n     15 years (number of years as a smoker)\\r\\n     16 fbs: (fasting blood sugar > 120 mg/dl)  (1 = true; 0 = false)\\r\\n     17 dm (1 = history of diabetes; 0 = no such history)\\r\\n     18 famhist: family history of coronary artery disease (1 = yes; 0 = no)\\r\\n     19 restecg: resting electrocardiographic results\\r\\n        -- Value 0: normal\\r\\n        -- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\\r\\n        -- Value 2: showing probable or definite left ventricular hypertrophy by Estes\\' criteria\\r\\n     20 ekgmo (month of exercise ECG reading)\\r\\n     21 ekgday(day of exercise ECG reading)\\r\\n     22 ekgyr (year of exercise ECG reading)\\r\\n     23 dig (digitalis used furing exercise ECG: 1 = yes; 0 = no)\\r\\n     24 prop (Beta blocker used during exercise ECG: 1 = yes; 0 = no)\\r\\n     25 nitr (nitrates used during exercise ECG: 1 = yes; 0 = no)\\r\\n     26 pro (calcium channel blocker used during exercise ECG: 1 = yes; 0 = no)\\r\\n     27 diuretic (diuretic used used during exercise ECG: 1 = yes; 0 = no)\\r\\n     28 proto: exercise protocol\\r\\n          1 = Bruce     \\r\\n          2 = Kottus\\r\\n          3 = McHenry\\r\\n          4 = fast Balke\\r\\n          5 = Balke\\r\\n          6 = Noughton \\r\\n          7 = bike 150 kpa min/min  (Not sure if \"kpa min/min\" is what was written!)\\r\\n          8 = bike 125 kpa min/min  \\r\\n          9 = bike 100 kpa min/min\\r\\n         10 = bike 75 kpa min/min\\r\\n         11 = bike 50 kpa min/min\\r\\n         12 = arm ergometer\\r\\n     29 thaldur: duration of exercise test in minutes\\r\\n     30 thaltime: time when ST measure depression was noted\\r\\n     31 met: mets achieved\\r\\n     32 thalach: maximum heart rate achieved\\r\\n     33 thalrest: resting heart rate\\r\\n     34 tpeakbps: peak exercise blood pressure (first of 2 parts)\\r\\n     35 tpeakbpd: peak exercise blood pressure (second of 2 parts)\\r\\n     36 dummy\\r\\n     37 trestbpd: resting blood pressure\\r\\n     38 exang: exercise induced angina (1 = yes; 0 = no)\\r\\n     39 xhypo: (1 = yes; 0 = no)\\r\\n     40 oldpeak = ST depression induced by exercise relative to rest\\r\\n     41 slope: the slope of the peak exercise ST segment\\r\\n        -- Value 1: upsloping\\r\\n        -- Value 2: flat\\r\\n        -- Value 3: downsloping\\r\\n     42 rldv5: height at rest\\r\\n     43 rldv5e: height at peak exercise\\r\\n     44 ca: number of major vessels (0-3) colored by flourosopy\\r\\n     45 restckm: irrelevant\\r\\n     46 exerckm: irrelevant\\r\\n     47 restef: rest raidonuclid (sp?) ejection fraction\\r\\n     48 restwm: rest wall (sp?) motion abnormality\\r\\n        0 = none\\r\\n        1 = mild or moderate\\r\\n        2 = moderate or severe\\r\\n        3 = akinesis or dyskmem (sp?)\\r\\n     49 exeref: exercise radinalid (sp?) ejection fraction\\r\\n     50 exerwm: exercise wall (sp?) motion \\r\\n     51 thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\\r\\n     52 thalsev: not used\\r\\n     53 thalpul: not used\\r\\n     54 earlobe: not used\\r\\n     55 cmo: month of cardiac cath (sp?)  (perhaps \"call\")\\r\\n     56 cday: day of cardiac cath (sp?)\\r\\n     57 cyr: year of cardiac cath (sp?)\\r\\n     58 num: diagnosis of heart disease (angiographic disease status)\\r\\n        -- Value 0: < 50% diameter narrowing\\r\\n        -- Value 1: > 50% diameter narrowing\\r\\n        (in any major vessel: attributes 59 through 68 are vessels)\\r\\n     59 lmt\\r\\n     60 ladprox\\r\\n     61 laddist\\r\\n     62 diag\\r\\n     63 cxmain\\r\\n     64 ramus\\r\\n     65 om1\\r\\n     66 om2\\r\\n     67 rcaprox\\r\\n     68 rcadist\\r\\n     69 lvx1: not used\\r\\n     70 lvx2: not used\\r\\n     71 lvx3: not used\\r\\n     72 lvx4: not used\\r\\n     73 lvf: not used\\r\\n     74 cathef: not used\\r\\n     75 junk: not used\\r\\n     76 name: last name of patient  (I replaced this with the dummy string \"name\")',\n",
       "  'citation': None}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_disease.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's get to know our dataset a little more:\n",
    "\n",
    "- age: Age (in years)\n",
    "- sex (0 for male, 1 for female)\n",
    "- cp: Chest pain type:\n",
    "  - 1: Typical angina\n",
    "  - 2: Atypical angina\n",
    "  - 3: Non-anginal pain\n",
    "  - 4: Asymptomatic\n",
    "- trestbps: Resting blood pressure (in mm Hg on admission to the hospital)\n",
    "- chol: Serum cholesterol (mm/Hg)\n",
    "- fbs: Fasting blood sugar > 120 mg/dl (1=True, 0=False)\n",
    "- restecg: resting electrocardiographic results (0: Normal, 1: Abnormal)\n",
    "- thalah: Maximum heart rate achieved\n",
    "- exang: Exercise induced angina (1=yes, 0=no)\n",
    "- oldpeak: ST depression induced by exercise relative to rest\n",
    "- slope: The slope of the peak exercise ST segment:\n",
    "  - 1: upsloping\n",
    "  - 2: flat\n",
    "  - 3: downsloping\n",
    "- ca: Number of major vessels (0-3) colored by flourosopy\n",
    "- Thal:\n",
    "  - 3: normal\n",
    "  - 6: fixed defect\n",
    "  - 7: reversable defect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and handling duplicates\n",
    "\n",
    "Most of the ML algorithms do not like missing values! Most of them will break with the presence of missing values in the features, or will drop the observations with the missing values!\n",
    "\n",
    "In ML there are two ways you can treat missing values. \n",
    "\n",
    "- Drop the rows/columns with the missing observations\n",
    "- Impute the missing value based on a metric (eg mean, median, ...) or a more complex method (eg KNN, Linear regression, ...)\n",
    "\n",
    "The selection of method dependds on the percerntage of missing values you have on your dataset, and also about the importance of the columns containing the missing values! For this tutorial, we will proceed with dropping the columns/rows with missing obseravtions!\n",
    "\n",
    "To make sure that we do not have any missing values, we will check our dataset usng the `is.na()` function from pandas! And then we can use the `.dropna()` function to remove the rows with the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       "thalach     0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          4\n",
       "thal        2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Isolate the predictors\n",
    "x = dataset.features\n",
    "\n",
    "# Check for missing values\n",
    "x_isna = x.isna()\n",
    "\n",
    "# is.na() returns a matrix with TRUE/FALSE, depending on if the obesravtion is missing or not. To get a sense of what's going on, we need to sum per column\n",
    "x_isna.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of observations with missing values, we can safely remove the observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(297, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the rows withthe missing values\n",
    "x_cln = x.dropna(axis=0)\n",
    "x_cln.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the same chacks for our tarhet variable as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in the target: num    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y = dataset.targets\n",
    "y = y.replace({2:1, 3:1, 4:1})\n",
    "n_na_tar = y.isna().sum()\n",
    "print(f'Number of missing values in the target: {n_na_tar}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in the process is to check for duplicates. It's not uncommon for datasets to include observations that are repeated. We can check for duplicates usnbe the `.duplicated()` method in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows in a dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates by adding the number or TRUE, which symbolises duplictaed rows.\n",
    "n_dups = x_cln.duplicated().sum()\n",
    "print(f'Number of duplicated rows in a dataset: {n_dups}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "#Now that our dataset is free from missing values and duplicates, we can start exploring our dataset so we can understand better the relationships between the features, and also relationships between #the features and the target.\n",
    "\n",
    "#EDA is a long process, especially when the dataset is new and/or you have no domain specific information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Now that our dataset is free from missing values and duplicates, we can start thinking about how to transform our features in a way that can be correctly used by the machine learning algorithms. We will use different techniques for categorical and numerical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features\n",
    "\n",
    "Almost every machine learning algorithm in Python will require some kind of transformation of the categorical features. While there are multiple ways of transforming categorical variables, the most widely used method is called **On-hot encoding**! \n",
    "\n",
    "In this method, each level of the categorical feature is represented by its own column, with the value 1 of the level is present in that observation and 0 when it's absent\n",
    "\n",
    "![ml_buldozer](img/one_hot_encoding.png)\n",
    "\n",
    "[*Source*](https://www.statology.org/one-hot-encoding-in-python/)\n",
    "\n",
    "**Question**: What do we do with the binary values? (eg Cat/Dog)?\n",
    "\n",
    "### One-hot encoding in Python\n",
    "\n",
    "There are 2 ways to perform one-hot encoding in Python. We can use either:\n",
    "\n",
    "- the pandas package\n",
    "- the scikit-learn package\n",
    "\n",
    "Pandas is more easy to use, while scikit learn is more easily combined with the rest of the ML algorithms (more to that, later today!)\n",
    "\n",
    "#### OHE in Pandas\n",
    "\n",
    "Pandas has the built-it function `get_dummies()` that performs the OHE operation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
       "       'exang', 'oldpeak', 'slope', 'ca', 'thal'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform OHE using pandas.get_dummies()\n",
    "import pandas as pd\n",
    "\n",
    "dataset_ohe = pd.get_dummies(x_cln)\n",
    "dataset_ohe.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pd.get_dummies()` accepts a dataframe will both categorical and numerical variables, but willl **only transform the categorical variables!** Because data types are tricky in pandas, I'd reccomend to always use *columns* argument to specify which columns you want to encode.\n",
    "\n",
    "As the following arguments as well:\n",
    "\n",
    "- *sparse = False*: Unless you have a huge dataset (which will contain millions of 0's), I'd reccomend to return a normal dataframe rather than a sparse one\n",
    "- *dummy_na* in case you want your NAs to be a label on their own\n",
    "- *drop_first = True* will be usefull in the ML setting\n",
    "\n",
    "Let's take a subset of our dataframe and perform this *the right-way*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tq/8nt7c8ld587csq8jpk2dycbr0000gn/T/ipykernel_62469/2830738718.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  pd.get_dummies(dataset_sub, columns=[\"sex\", \"cp\"], drop_first=True, sparse=False).replace({False:0, True:1})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>slope</th>\n",
       "      <th>sex_1</th>\n",
       "      <th>cp_2</th>\n",
       "      <th>cp_3</th>\n",
       "      <th>cp_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>297 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  slope  sex_1  cp_2  cp_3  cp_4\n",
       "0     63      3      1     0     0     0\n",
       "1     67      2      1     0     0     1\n",
       "2     67      2      1     0     0     1\n",
       "3     37      3      1     0     1     0\n",
       "4     41      1      0     1     0     0\n",
       "..   ...    ...    ...   ...   ...   ...\n",
       "297   57      2      0     0     0     1\n",
       "298   45      2      1     0     0     0\n",
       "299   68      2      1     0     0     1\n",
       "300   57      2      1     0     0     1\n",
       "301   57      2      0     1     0     0\n",
       "\n",
       "[297 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sub = x_cln.loc[:,[\"age\", \"sex\", \"cp\", \"slope\"]]\n",
    "pd.get_dummies(dataset_sub, columns=[\"sex\", \"cp\"], drop_first=True, sparse=False).replace({False:0, True:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OHE in Scikit-learn\n",
    "\n",
    "Scikit-learn has another way to perform OHE via the the `OneHotEncoder` function. All scikit-learn's operations are implementted the following way:\n",
    "\n",
    "- **initiate**: Instantiate the object\n",
    "- **fit**: *Train* the object using the data\n",
    "- **transform**: Transform the data\n",
    "\n",
    "Similar aeguments to have allok at:\n",
    "\n",
    "- *drop*\n",
    "- *sparse_output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "ohe.fit(dataset_sub)\n",
    "ohe.transform(dataset_sub)\n",
    "\n",
    "# You can concatinate this in one line using\n",
    "ohe.fit_transform(dataset_sub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that the outcome of the the scikit-learn's version is a `np.array` rather than a pd.Dataframe, and that's fine for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical variables in Python\n",
    "\n",
    "Numerical variables are easier to deal with when preparing a dataset for a machine learning project. While most of the ML algorithms do not require any transformation before applying, some of them work better after transformaiton. The transformation I recocment is the **standardization**\n",
    "\n",
    "Standardizing a feature happens when we substract the mean and we devide by the standard deviation:\n",
    "\n",
    "$$\n",
    "\\frac{x_i - \\bar{x}}{s_x}\n",
    "$$\n",
    "\n",
    "While it's easy to do this transformation using pandas, we will use the scikit-learn's implementtation in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.93618065,  0.69109474, -2.24062879,  2.26414539],\n",
       "       [ 1.3789285 ,  0.69109474,  0.87388018,  0.6437811 ],\n",
       "       [ 1.3789285 ,  0.69109474,  0.87388018,  0.6437811 ],\n",
       "       ...,\n",
       "       [ 1.48961547,  0.69109474,  0.87388018,  0.6437811 ],\n",
       "       [ 0.27205887,  0.69109474,  0.87388018,  0.6437811 ],\n",
       "       [ 0.27205887, -1.44697961, -1.20245913,  0.6437811 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scl = StandardScaler()\n",
    "scl.fit(dataset_sub)\n",
    "scl.transform(dataset_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One good thing about scikit learn is that all the functionality is applied the same way, regardless if it's a processing step, or a ML learning algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intorduction to Supervised Learning in Python\n",
    "\n",
    "Most of the problems that we face in ML fall into the category of supervised learning. This means that we want either to assign a numerical value to an observation (regression) or assign it to one of the pre-defined classes (classification). We do that by having a *label* associated with each observation, from which we can use a model to find the relationships between the label (also known as response or target), and the features.\n",
    "\n",
    "![supervised_learning](img/supervised_learning.webp)\n",
    "\n",
    "[*Source*](https://www.kdnuggets.com/understanding-supervised-learning-theory-and-overview)\n",
    "\n",
    "But.. how do we assure that our algorithm performs as expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split in Python\n",
    "\n",
    "Train test split is a model validation procedure that allows you to simulate how a model would perform on new/unseen data. Here is how the procedure works:\n",
    "\n",
    "![supervised_learning](img/train_test_split.jpg)\n",
    "\n",
    "[*Source*](https://builtin.com/data-science/train-test-split)\n",
    "\n",
    "In python, we can use the `train_test_split()` like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 17, 19, 22, 23, 24, 25, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 45, 46, 47, 48, 50, 51, 52, 53, 55, 56, 57, 58, 60, 61, 62, 64, 65, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 82, 83, 84, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 110, 111, 113, 114, 115, 116, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 143, 145, 146, 147, 149, 151, 152, 153, 154, 155, 157, 158, 159, 160, 161, 163, 164, 166, 168, 170, 171, 172, 173, 174, 176, 177, 178, 179, 181, 183, 184, 185, 186, 187, 189, 191, 193, 194, 196, 197, 198, 199, 200, 201, 203, 204, 205, 207, 208, 209, 210, 212, 214, 215, 216, 219, 220, 224, 225, 226, 227, 229, 230, 234, 235, 236, 237, 239, 241, 243, 245, 246, 247, 251, 252, 253, 254, 255, 256, 257, 259, 260, 261, 262, 264, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 286, 287, 288, 291, 292, 293, 295, 296, 297, 299, 300, 302]\n",
      "[0, 1, 3, 13, 16, 18, 20, 21, 26, 31, 38, 44, 49, 54, 59, 63, 66, 67, 77, 81, 85, 86, 95, 100, 108, 109, 112, 117, 138, 144, 148, 150, 156, 162, 165, 167, 169, 175, 180, 182, 188, 190, 192, 195, 202, 206, 211, 213, 217, 218, 221, 222, 223, 228, 231, 232, 233, 238, 240, 242, 244, 248, 249, 250, 258, 263, 265, 266, 275, 284, 285, 289, 290, 294, 298, 301]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "print(sorted(x_train.index))\n",
    "print(sorted(x_test.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few arguments to keep in mind while using this function:\n",
    "\n",
    "- *train/test_size* will determine the proportion of your test or train set (default is 70/30 I think...)\n",
    "- *stratify = True* will make sure that the proportion of lables in the train and test set are equal (for discrete labels)\n",
    "- *random_state* to ensure reproducability\n",
    "\n",
    "Now we can safely train on the train set and then asses the performance of our algorithm in the test set! Case closed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "What if we happen to just take a good/bad proportion of data to train on? An idea is to take a few splits\n",
    "\n",
    "![supervised_learning](img/kfold_cv.webp)\n",
    "\n",
    "[*Source*](https://towardsdatascience.com/cross-validation-k-fold-vs-monte-carlo-e54df2fc179b)\n",
    "\n",
    "In python, we can create folds like that using the `KFold` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: [101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
      " 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136\n",
      " 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154\n",
      " 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172\n",
      " 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190\n",
      " 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208\n",
      " 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226\n",
      " 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244\n",
      " 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262\n",
      " 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280\n",
      " 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298\n",
      " 299 300 301 302]\n",
      "Test set: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100]\n",
      "Train set: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 202 203 204 205 206 207 208\n",
      " 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226\n",
      " 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244\n",
      " 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262\n",
      " 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280\n",
      " 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298\n",
      " 299 300 301 302]\n",
      "Test set: [101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
      " 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136\n",
      " 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154\n",
      " 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172\n",
      " 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190\n",
      " 191 192 193 194 195 196 197 198 199 200 201]\n",
      "Train set: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201]\n",
      "Test set: [202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219\n",
      " 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237\n",
      " 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255\n",
      " 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273\n",
      " 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291\n",
      " 292 293 294 295 296 297 298 299 300 301 302]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv = KFold(n_splits = 3)\n",
    "for train ,test in cv.split(x,y):\n",
    "    print(f\"Train set: {train}\")\n",
    "    print(f\"Test set: {test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its a good time to put them all together and train our first supervised machine learning model: A logisitc regressioh model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a ML model\n",
    "\n",
    "We'll put totgether all the steps we implemented before. I fact we will:\n",
    "\n",
    "1. One-hot encode the categorical variables on the training data\n",
    "2. Standardize the numerical variables on the test data\n",
    "3. Train the model using the training data\n",
    "4. Apply the transformations on the test data\n",
    "5. Test the trained model on the test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leosouliotis/Documents/ODSC2024/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import set_config\n",
    "set_config(transform_output = \"pandas\")\n",
    "\n",
    "# Take a subset of the data\n",
    "numerical_vars = [\"age\"]\n",
    "categorical_vars = [\"sex\", \"cp\", \"slope\"]\n",
    "x = x[numerical_vars + categorical_vars]\n",
    "\n",
    "# Split the data into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, stratify=y)\n",
    "\n",
    "# Numerical transformations\n",
    "scl = StandardScaler()\n",
    "scl.fit(x_train[numerical_vars])\n",
    "x_num_tr = scl.transform(x_train[numerical_vars])\n",
    "\n",
    "# Categorical transformations\n",
    "ohe = OneHotEncoder(sparse_output=False, drop=\"first\")\n",
    "ohe.fit(x_train[categorical_vars])\n",
    "x_cat_tr = ohe.transform(x_train[categorical_vars])\n",
    "\n",
    "# Merge the categorical and numerical features\n",
    "x_train = pd.merge(x_num_tr, x_cat_tr, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "# Train the logistic regressio model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train, y_train)\n",
    "\n",
    "# Transform the test data based on the transformation of the train data\n",
    "x_num_test = scl.transform(x_test[numerical_vars])\n",
    "x_cat_test = ohe.transform(x_test[categorical_vars])\n",
    "x_test = pd.merge(x_num_test, x_cat_test, left_index=True, right_index=True)\n",
    "\n",
    "# Predict using the model on the test data\n",
    "predicted_probs = logreg.predict_proba(x_test)[:,1]\n",
    "predictions = [1 if prob > 0.5 else 0 for prob in predicted_probs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to evaluate a ML algorithm\n",
    "\n",
    "Now that we have the results from the test set, we can put them against the true values (rememeber, we know the true labels of the test set) using differnt metrics. Some of them are listed here:\n",
    "\n",
    "![metrics](img/metrics.png)\n",
    "\n",
    "[*Source*](https://learnanalyticshere.wordpress.com/2021/02/06/machine-learning-easy-reference/)\n",
    "\n",
    "and we can use them in python by using the respecitve functions in python \n",
    "\n",
    "- Accuracy: `sklearn.metrics.accuracy`\n",
    "- Precision: `sklearn.metrics.precision_score`\n",
    "- Recall: `sklearn.metrics.recall_score`\n",
    "- F1-score: `sklearn.metrics.f1_score`\n",
    "\n",
    "For this exmaple we will use the F1-score as a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8813559322033898)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_test_score = f1_score(y_test, predictions)\n",
    "f1_test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what about the K-Fold case, where we can have multiple test sets? Do not worry, is as easy as wrting a *for loop* over what we did, while being a bit careful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean F1 score accross folds is:0.7759533311162842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leosouliotis/Documents/ODSC2024/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/leosouliotis/Documents/ODSC2024/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/leosouliotis/Documents/ODSC2024/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_scores = []\n",
    "\n",
    "for train_idx, test_idx in cv.split(x,y):\n",
    "\n",
    "    # Split the data into train and test\n",
    "    x_train = x.iloc[train_idx]\n",
    "    x_test = x.iloc[test_idx]\n",
    "    y_train = y.iloc[train_idx]\n",
    "    y_test = y.iloc[test_idx]\n",
    "\n",
    "    # Numerical transformations\n",
    "    scl = StandardScaler()\n",
    "    scl.fit(x_train[numerical_vars])\n",
    "    x_num_tr = scl.transform(x_train[numerical_vars])\n",
    "\n",
    "    # Categorical transformations\n",
    "    ohe = OneHotEncoder(sparse_output=False, drop=\"first\")\n",
    "    ohe.fit(x_train[categorical_vars])\n",
    "    x_cat_tr = ohe.transform(x_train[categorical_vars])\n",
    "\n",
    "    # Merge the categorical and numerical features\n",
    "    x_train = pd.merge(x_num_tr, x_cat_tr, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "    # Train the logistic regressio model\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(x_train, y_train)\n",
    "\n",
    "    # Transform the test data based on the transformation of the train data\n",
    "    x_num_test = scl.transform(x_test[numerical_vars])\n",
    "    x_cat_test = ohe.transform(x_test[categorical_vars])\n",
    "    x_test = pd.merge(x_num_test, x_cat_test, left_index=True, right_index=True)\n",
    "\n",
    "    # Predict using the model on the test data\n",
    "    predicted_probs = logreg.predict_proba(x_test)[:,1]\n",
    "    predictions = [1 if prob > 0.5 else 0 for prob in predicted_probs]\n",
    "    all_scores.append(f1_score(y_test, predictions))\n",
    "\n",
    "print(f\"The mean F1 score accross folds is:{np.mean(all_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more instersting case: training a Random Forest model!\n",
    "\n",
    "We did it, we split the data into test and train, in more than one ways, we trained a ML model from scratch and we did asses its performance using a few metrics.\n",
    "\n",
    "But do we have to write the exact same loop every time we want to do a cross validation? Let's see a diferent approach using scikit learn, and of course, a different model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together using columnTransformers and pipelines\n",
    "\n",
    "`pipeline` is a tool for building structured workflows that seamlessly combine data preprocessing, transformation, and model training into a single, streamlined process. A pipeline organizes a series of operations into a defined sequence, ensuring each step is applied consistently and correctly across both training and testing datasets.\n",
    "\n",
    "Creating a pipeline is very easy, as everything in scikit-learn! The only two components we need is an actual transformers (as the ones we have used before) and a name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"scaler\", \n",
    "            StandardScaler()\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"onehot\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, drop=\"first\"),\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These transformers are a simple example; you can use any other transformartion you want as part of your pipelines. One transformer you can pair these existing pipeliines is how to handle missing data. You could possibly have:\n",
    "\n",
    "- An extra step in the `numeric_transformer` to impute missing values (eg with the median)\n",
    "- An extra step in the `categorical_transformer` to repalce missing values with \"Unknown\", ths creating an extra column while One-Hot Encoding\n",
    "\n",
    "And the way these pipleines are used is the same way as their underlying transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.936181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.378929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.378929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.941680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.498933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.272059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-1.056185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1.489615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.272059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.272059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>297 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age\n",
       "0    0.936181\n",
       "1    1.378929\n",
       "2    1.378929\n",
       "3   -1.941680\n",
       "4   -1.498933\n",
       "..        ...\n",
       "297  0.272059\n",
       "298 -1.056185\n",
       "299  1.489615\n",
       "300  0.272059\n",
       "301  0.272059\n",
       "\n",
       "[297 rows x 1 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_transformer.fit_transform(x_cln[[\"age\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have set different pipelines for the different kinds of variables, it's time to combine them into one single transformer! Scikit-learn has covered us on this one, using the `ColumnTransformer` function! In `ColumnTransformer` we are dictating which transformers to be used on which columns, plus giving them a name (more on why later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num__age</th>\n",
       "      <th>cat__sex_1</th>\n",
       "      <th>cat__cp_2</th>\n",
       "      <th>cat__cp_3</th>\n",
       "      <th>cat__cp_4</th>\n",
       "      <th>cat__slope_2</th>\n",
       "      <th>cat__slope_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.936181</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.378929</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.378929</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.941680</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.498933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.272059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-1.056185</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1.489615</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.272059</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.272059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>297 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     num__age  cat__sex_1  cat__cp_2  cat__cp_3  cat__cp_4  cat__slope_2  \\\n",
       "0    0.936181         1.0        0.0        0.0        0.0           0.0   \n",
       "1    1.378929         1.0        0.0        0.0        1.0           1.0   \n",
       "2    1.378929         1.0        0.0        0.0        1.0           1.0   \n",
       "3   -1.941680         1.0        0.0        1.0        0.0           0.0   \n",
       "4   -1.498933         0.0        1.0        0.0        0.0           0.0   \n",
       "..        ...         ...        ...        ...        ...           ...   \n",
       "297  0.272059         0.0        0.0        0.0        1.0           1.0   \n",
       "298 -1.056185         1.0        0.0        0.0        0.0           1.0   \n",
       "299  1.489615         1.0        0.0        0.0        1.0           1.0   \n",
       "300  0.272059         1.0        0.0        0.0        1.0           1.0   \n",
       "301  0.272059         0.0        1.0        0.0        0.0           1.0   \n",
       "\n",
       "     cat__slope_3  \n",
       "0             1.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             1.0  \n",
       "4             0.0  \n",
       "..            ...  \n",
       "297           0.0  \n",
       "298           0.0  \n",
       "299           0.0  \n",
       "300           0.0  \n",
       "301           0.0  \n",
       "\n",
       "[297 rows x 7 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Combine preprocessing steps using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numerical_vars),\n",
    "        (\"cat\", categorical_transformer, categorical_vars),\n",
    "    ]\n",
    ")\n",
    "preprocessor.fit_transform(dataset_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines are so versatile that allow us to combine all the transformers we defined with a machine learning model! In this example, we pair it with a [Random Forest](https://en.wikipedia.org/wiki/Random_forest) model. \n",
    "\n",
    "And a little reminder on how Random Forests work\n",
    "\n",
    "![metrics](img/random_forest.webp)\n",
    "\n",
    "[*Source*](https://medium.com/@denizgunay/random-forest-af5bde5d7e1e)\n",
    "\n",
    "Let's combine them all together now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "ml_pipeline = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\",RandomForestClassifier())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `ml_pipeline` knows everything we want to do with our data! We have declared what to do with the data (and in which columns) and which classifier to use! \n",
    "\n",
    "The reason why we did all this work to combine all these pre-processing steps is so we can automate the train/test *for loops* we used before, with the sklearn function `cross_val_score()`! By providing the:\n",
    "\n",
    "- classifier/regressor (or in our case, the pipeline)\n",
    "- features\n",
    "- target\n",
    "- CV \n",
    "- metric to be used\n",
    "\n",
    "the function will spit out the accross all the folds! One cool thing is that it auto-magically trains the transformers/classifier on the training sets (as dictated by the cv) and then predicts on the test set!\n",
    "\n",
    "But let's see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77272727, 0.71428571, 0.74725275])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores_cv = cross_val_score(ml_pipeline, x, y.values.ravel(), cv=cv, scoring=\"f1\")\n",
    "scores_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's way better than a long for loop, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to tune a complex model? \n",
    "\n",
    "While logisitc regression is a relativley simple model, random forests are not. That means that many decisions need to be made about some non-trainable parameters (called *hyper-parameters*) that will be decided before the model is trained. These include:\n",
    "\n",
    "- *max_depth*: The maximum depth of the tree\n",
    "- *n_estimators*: The numbers of trees in the forests\n",
    "- *max_features*: The % of variables to be used\n",
    "\n",
    "But how do we know which are the *optimal* value of these parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most intuitive way to achieve this is to use all the possible combinations of the hyper-parameters and then choose the model with the highest testing performance! This should add another *for loop* in our operations, right?\n",
    "\n",
    "Sklearn have us covered, using the `GridSearchCV` functionality! It works the same way as the `cross_val_score`, but with the addition of a *grid*; a dictionary of all potential values for all the *hyper-parameters*. \n",
    "\n",
    "Let's construct our grid first and then put it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'classifier__max_depth': [2,3],\n",
    "    'classifier__n_estimators': [10, 50, 100],  \n",
    "    'classifier__max_features': [0.75, 1]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the naming convention of the *hyper-parameters*? This specifies which part of the pipeline the  hyperparameter belong to. This happens so that you can try combination with hyper parameters that belong to the transformers as well!\n",
    "\n",
    "You could, for examle, compare differnt imputation methods or what the effects of creating an extra column in OHE for your unkowns vs dropping them! Even thought this falls outside the socpe of this tutorial, you can combine *hyper-parameters* for every part of your classifier!\n",
    "\n",
    "Now let's focus on the grid created for the Random Forests and fit it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-6 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-6 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-6 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-6 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-6 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-6 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-6 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-6 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-6 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                                        ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                                                         Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         [&#x27;age&#x27;]),\n",
       "                                                                        (&#x27;cat&#x27;,\n",
       "                                                                         Pipeline(steps=[(&#x27;onehot&#x27;,\n",
       "                                                                                          OneHotEncoder(drop=&#x27;first&#x27;,\n",
       "                                                                                                        handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                                                        sparse_output=False))]),\n",
       "                                                                         [&#x27;sex&#x27;,\n",
       "                                                                          &#x27;cp&#x27;,\n",
       "                                                                          &#x27;slope&#x27;])])),\n",
       "                                       (&#x27;classifier&#x27;,\n",
       "                                        RandomForestClassifier())]),\n",
       "             param_grid={&#x27;classifier__max_depth&#x27;: [2, 3],\n",
       "                         &#x27;classifier__max_features&#x27;: [0.75, 1],\n",
       "                         &#x27;classifier__n_estimators&#x27;: [10, 50, 100]},\n",
       "             scoring=&#x27;f1&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-34\" type=\"checkbox\" ><label for=\"sk-estimator-id-34\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                                        ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                                                         Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         [&#x27;age&#x27;]),\n",
       "                                                                        (&#x27;cat&#x27;,\n",
       "                                                                         Pipeline(steps=[(&#x27;onehot&#x27;,\n",
       "                                                                                          OneHotEncoder(drop=&#x27;first&#x27;,\n",
       "                                                                                                        handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                                                        sparse_output=False))]),\n",
       "                                                                         [&#x27;sex&#x27;,\n",
       "                                                                          &#x27;cp&#x27;,\n",
       "                                                                          &#x27;slope&#x27;])])),\n",
       "                                       (&#x27;classifier&#x27;,\n",
       "                                        RandomForestClassifier())]),\n",
       "             param_grid={&#x27;classifier__max_depth&#x27;: [2, 3],\n",
       "                         &#x27;classifier__max_features&#x27;: [0.75, 1],\n",
       "                         &#x27;classifier__n_estimators&#x27;: [10, 50, 100]},\n",
       "             scoring=&#x27;f1&#x27;)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-35\" type=\"checkbox\" ><label for=\"sk-estimator-id-35\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">best_estimator_: Pipeline</label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  [&#x27;age&#x27;]),\n",
       "                                                 (&#x27;cat&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;onehot&#x27;,\n",
       "                                                                   OneHotEncoder(drop=&#x27;first&#x27;,\n",
       "                                                                                 handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                                 sparse_output=False))]),\n",
       "                                                  [&#x27;sex&#x27;, &#x27;cp&#x27;, &#x27;slope&#x27;])])),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 RandomForestClassifier(max_depth=3, max_features=0.75,\n",
       "                                        n_estimators=10))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-36\" type=\"checkbox\" ><label for=\"sk-estimator-id-36\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;preprocessor: ColumnTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for preprocessor: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler())]),\n",
       "                                 [&#x27;age&#x27;]),\n",
       "                                (&#x27;cat&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;onehot&#x27;,\n",
       "                                                  OneHotEncoder(drop=&#x27;first&#x27;,\n",
       "                                                                handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse_output=False))]),\n",
       "                                 [&#x27;sex&#x27;, &#x27;cp&#x27;, &#x27;slope&#x27;])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-37\" type=\"checkbox\" ><label for=\"sk-estimator-id-37\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">num</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;age&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-38\" type=\"checkbox\" ><label for=\"sk-estimator-id-38\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-39\" type=\"checkbox\" ><label for=\"sk-estimator-id-39\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">cat</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;sex&#x27;, &#x27;cp&#x27;, &#x27;slope&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-40\" type=\"checkbox\" ><label for=\"sk-estimator-id-40\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder(drop=&#x27;first&#x27;, handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre></div> </div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-41\" type=\"checkbox\" ><label for=\"sk-estimator-id-41\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(max_depth=3, max_features=0.75, n_estimators=10)</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('preprocessor',\n",
       "                                        ColumnTransformer(transformers=[('num',\n",
       "                                                                         Pipeline(steps=[('scaler',\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         ['age']),\n",
       "                                                                        ('cat',\n",
       "                                                                         Pipeline(steps=[('onehot',\n",
       "                                                                                          OneHotEncoder(drop='first',\n",
       "                                                                                                        handle_unknown='ignore',\n",
       "                                                                                                        sparse_output=False))]),\n",
       "                                                                         ['sex',\n",
       "                                                                          'cp',\n",
       "                                                                          'slope'])])),\n",
       "                                       ('classifier',\n",
       "                                        RandomForestClassifier())]),\n",
       "             param_grid={'classifier__max_depth': [2, 3],\n",
       "                         'classifier__max_features': [0.75, 1],\n",
       "                         'classifier__n_estimators': [10, 50, 100]},\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(ml_pipeline, param_grid, scoring=\"f1\", refit=True)\n",
    "grid_search.fit(x,y.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now that all have came together, lets explore this `grid_search` object! The key atribute is `.cv_results_` which sumamarizes everything the object has captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01315045, 0.02413034, 0.04445066, 0.00683346, 0.02301197,\n",
       "        0.04257693, 0.00716472, 0.02438636, 0.04537168, 0.00690422,\n",
       "        0.02337527, 0.04306636]),\n",
       " 'std_fit_time': array([0.00918085, 0.00046424, 0.00035663, 0.00026913, 0.00031273,\n",
       "        0.0002024 , 0.00015765, 0.00059123, 0.00038773, 0.00010996,\n",
       "        0.00019654, 0.00043401]),\n",
       " 'mean_score_time': array([0.00296817, 0.00317569, 0.00373278, 0.00240221, 0.0030776 ,\n",
       "        0.00387626, 0.00238824, 0.00305638, 0.00368462, 0.00233698,\n",
       "        0.00318909, 0.00381503]),\n",
       " 'std_score_time': array([5.09752228e-04, 3.86037021e-04, 1.79815261e-04, 1.42607207e-04,\n",
       "        1.60145289e-04, 7.90804489e-05, 1.21369155e-04, 2.53724268e-04,\n",
       "        1.26439610e-04, 1.79995701e-04, 2.36893401e-04, 2.90767526e-04]),\n",
       " 'param_classifier__max_depth': masked_array(data=[2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value=999999),\n",
       " 'param_classifier__max_features': masked_array(data=[0.75, 0.75, 0.75, 1.0, 1.0, 1.0, 0.75, 0.75, 0.75, 1.0,\n",
       "                    1.0, 1.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value=1e+20),\n",
       " 'param_classifier__n_estimators': masked_array(data=[10, 50, 100, 10, 50, 100, 10, 50, 100, 10, 50, 100],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value=999999),\n",
       " 'params': [{'classifier__max_depth': 2,\n",
       "   'classifier__max_features': 0.75,\n",
       "   'classifier__n_estimators': 10},\n",
       "  {'classifier__max_depth': 2,\n",
       "   'classifier__max_features': 0.75,\n",
       "   'classifier__n_estimators': 50},\n",
       "  {'classifier__max_depth': 2,\n",
       "   'classifier__max_features': 0.75,\n",
       "   'classifier__n_estimators': 100},\n",
       "  {'classifier__max_depth': 2,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__n_estimators': 10},\n",
       "  {'classifier__max_depth': 2,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__n_estimators': 50},\n",
       "  {'classifier__max_depth': 2,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__n_estimators': 100},\n",
       "  {'classifier__max_depth': 3,\n",
       "   'classifier__max_features': 0.75,\n",
       "   'classifier__n_estimators': 10},\n",
       "  {'classifier__max_depth': 3,\n",
       "   'classifier__max_features': 0.75,\n",
       "   'classifier__n_estimators': 50},\n",
       "  {'classifier__max_depth': 3,\n",
       "   'classifier__max_features': 0.75,\n",
       "   'classifier__n_estimators': 100},\n",
       "  {'classifier__max_depth': 3,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__n_estimators': 10},\n",
       "  {'classifier__max_depth': 3,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__n_estimators': 50},\n",
       "  {'classifier__max_depth': 3,\n",
       "   'classifier__max_features': 1,\n",
       "   'classifier__n_estimators': 100}],\n",
       " 'split0_test_score': array([0.75      , 0.75      , 0.75      , 0.70588235, 0.73684211,\n",
       "        0.71428571, 0.74576271, 0.72413793, 0.74576271, 0.71186441,\n",
       "        0.66666667, 0.70175439]),\n",
       " 'split1_test_score': array([0.75862069, 0.75862069, 0.75862069, 0.71428571, 0.8       ,\n",
       "        0.77777778, 0.81967213, 0.79310345, 0.8       , 0.76923077,\n",
       "        0.8       , 0.75      ]),\n",
       " 'split2_test_score': array([0.73684211, 0.77966102, 0.73684211, 0.77966102, 0.76666667,\n",
       "        0.73684211, 0.81967213, 0.84375   , 0.84375   , 0.78571429,\n",
       "        0.78688525, 0.75409836]),\n",
       " 'split3_test_score': array([0.75      , 0.75      , 0.75      , 0.77192982, 0.74074074,\n",
       "        0.80701754, 0.78688525, 0.79310345, 0.79310345, 0.8       ,\n",
       "        0.81481481, 0.79245283]),\n",
       " 'split4_test_score': array([0.71428571, 0.71428571, 0.71428571, 0.71428571, 0.7037037 ,\n",
       "        0.71428571, 0.75862069, 0.74576271, 0.74576271, 0.67924528,\n",
       "        0.75      , 0.75      ]),\n",
       " 'mean_test_score': array([0.7419497 , 0.75051348, 0.7419497 , 0.73720892, 0.74959064,\n",
       "        0.75004177, 0.78612258, 0.77997151, 0.78567577, 0.74921095,\n",
       "        0.76367335, 0.74966112]),\n",
       " 'std_test_score': array([0.01548496, 0.02111059, 0.01548496, 0.03174909, 0.03218432,\n",
       "        0.03673349, 0.03045324, 0.04171384, 0.03692986, 0.04604948,\n",
       "        0.05304927, 0.0287962 ]),\n",
       " 'rank_test_score': array([10,  5, 10, 12,  8,  6,  1,  3,  2,  9,  4,  7], dtype=int32)}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this one seems a bit chaotic; that's why there are a few objects of interest:\n",
    "\n",
    "- `.best_params_`: The set of the best *hyper-parameters*\n",
    "- `.best_score`: The test score of the classifier using the best params\n",
    "- `.best_estimator`: As we defined *refit=True*, the best classifier is refitted using the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A\n",
    "\n",
    "I hope you enjoyed this tutorial on Introduction to Machine Learning using Python! If you have any question, that's thew best time to ask!\n",
    "\n",
    "Fire away!\n",
    "\n",
    "In case something comes to your mind later, you can reach me on leosouliotis@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
